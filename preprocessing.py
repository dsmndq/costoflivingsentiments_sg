import re
import nltk
import pandas as pd
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from tqdm import tqdm

def download_nltk_resources():
    """
    Checks for and downloads necessary NLTK data packages if they are missing.
    This makes the script more robust by handling setup automatically.
    """
    required_packages = {
        'tokenizers/punkt': 'punkt',
        'corpora/stopwords': 'stopwords',
        'corpora/wordnet': 'wordnet'
    }
    print("--- Checking for NLTK resources ---")
    for path, package_id in required_packages.items():
        try:
            nltk.data.find(path)
            print(f"[*] '{package_id}' resource is already available.")
        except LookupError:
            print(f"[*] Downloading missing resource: '{package_id}'...")
            nltk.download(package_id)
            print(f"[*] Download of '{package_id}' complete.")
    print("--- NLTK resource check complete ---\n")

# --- Pre-initialized objects for performance ---
# Create these objects once to avoid re-creating them on every function call,
# which is more efficient when processing large amounts of text.
LEMMATIZER = WordNetLemmatizer()

# Combine standard English stopwords with a custom list for the specific domain.
STOP_WORDS = set(stopwords.words('english'))
CUSTOM_STOPWORDS = {
    'singapore', 'sg', 'like', 'get', 'one', 'also', 'would', 'really', 'im', 'u'
}
STOP_WORDS.update(CUSTOM_STOPWORDS)
# ---

def preprocess_social_media_text(text: str) -> str:
    """
    Cleans and preprocesses raw text from social media for NLP tasks.

    This function executes a pipeline of text cleaning techniques specifically
    tailored for noisy, user-generated content like Reddit comments. The goal is
    to normalize the text and make it suitable for feature extraction and modeling.[1, 2]

    The pipeline includes:
    1.  Lowercasing: Converts all text to lowercase for consistency.[1, 3, 4]
    2.  Noise Removal: Uses regular expressions to remove URLs, Reddit-specific tags
        (e.g., u/username, r/subreddit), and any characters that are not letters
        or whitespace.[1, 3]
    3.  Tokenization: Splits the cleaned text into individual words (tokens).[3, 5]
    4.  Stopword Removal: Removes common English stopwords and a custom list of
        domain-specific words (e.g., 'singapore', 'sg') that add little
        discriminative value.[3, 5, 6]
    5.  Lemmatization: Reduces words to their base or root form (lemma). This is
        preferred over stemming as it results in actual words, which improves the
        interpretability of models like topic modeling.[1, 4, 5]

    Args:
        text (str): The raw text string to be processed.

    Returns:
        str: The cleaned and processed text as a single, space-separated string.
    """
    if not isinstance(text, str):
        return ""

    # Step 1: Lowercasing
    text = text.lower()

    # Step 2: Noise Removal with Regular Expressions
    # Remove URLs
    text = re.sub(r'https://\S+|http\S+|www\S+', '', text)
    # Remove Reddit user and subreddit mentions
    text = re.sub(r'u/\w+|r/\w+', '', text)
    # Remove all non-alphabetic characters (e.g., punctuation, numbers, emojis)
    # but keep whitespace to separate words.
    text = re.sub(r'[^a-z\s]', '', text)

    # Step 3: Whitespace Normalization and Tokenization
    # Collapse all whitespace (spaces, tabs, newlines) into a single space
    # and remove leading/trailing whitespace. This prevents the tokenizer
    # from misinterpreting special characters like tabs.
    text = re.sub(r'\s+', ' ', text).strip()
    # Split the text into a list of words.
    tokens = word_tokenize(text)

    # Filter out the stopwords from the token list.
    filtered_tokens = [word for word in tokens if word not in STOP_WORDS]

    # Step 5: Lemmatization
    # Reduce each word to its base form.
    lemmatized_tokens = [LEMMATIZER.lemmatize(word) for word in filtered_tokens]

    # Join the processed tokens back into a single string.
    processed_text = ' '.join(lemmatized_tokens)

    return processed_text

if __name__ == '__main__':
    # --- Setup: Ensure NLTK resources are available ---
    download_nltk_resources()

    # Initialize tqdm for pandas to show progress bars on .apply()
    tqdm.pandas(desc="Processing text")

    # --- Configuration ---
    # The single input file generated by the updated scraping.py script
    INPUT_FILE = 'scraped_relevant_comments_praw.csv'
    # The final output file ready for sentiment analysis or other NLP tasks
    OUTPUT_FILE = 'processed_corpus.csv'

    # --- Process the Scraped Data ---
    print(f"--- Starting to process {INPUT_FILE} ---")
    try:
        # Load the raw comments data
        corpus_df = pd.read_csv(INPUT_FILE)
        print(f"Loaded {len(corpus_df)} raw comments.")

        # Apply the preprocessing function to the 'raw_text' column.
        # .progress_apply shows a progress bar, which is helpful for large datasets.
        # .fillna('') handles any potentially empty comments before processing.
        corpus_df['cleaned_text'] = corpus_df['raw_text'].fillna('').progress_apply(preprocess_social_media_text)

        # Save the processed data to a new CSV file.
        corpus_df.to_csv(OUTPUT_FILE, index=False)
        print(f"\nSuccessfully processed and saved {len(corpus_df)} entries to {OUTPUT_FILE}")
        print("Sample of processed comments:")
        print(corpus_df[['raw_text', 'cleaned_text']].head())

    except FileNotFoundError:
        print(f"\nError: Input file not found at '{INPUT_FILE}'.")
        print("Please run the 'scraping.py' script first to generate the data.")
    except Exception as e:
        print(f"\nAn error occurred while processing the file: {e}")